folder=/Users/lenin/Dropbox/workspace-sts-3.4.0.RELEASE.macmini/crawler/output/
outFileNameSuffix=.rss.us.tt.first_time_level_1.
input_list_of_URL_in_a_file=/Users/lenin/Downloads/crawleroutput/output/all.think.tank.from.world.aug.20.2015/first_level_rss.world.tt.first_time.Already_crawled.merged.FILEID.2.and.3.txt
output_Already_Crawled_File=/Users/lenin/Downloads/crawleroutput/output/first_level_rss.world.tt.first_time.Already_crawled.FILEID.3.txt
start_tag=href
end_tag=>
FilterURLOnMatchedString=dummy
FilterURLOnMatchedString.remarks=retain only if this pattern matched.
FilterURL_Omit_OnMatchedString=dummy
FilterURL_Omit_OnMatchedString.remarks=Omit if this pattern matches
Stop_Word_for_ExtractedText=dummy
Stop_Word_for_ExtractedText.remarks=Once this pattern found, just terminate
skip_is_english=true
skip_is_english.remarks=true means it will consider all URLs as English based and go for crawling; false means it checks if given URL is english and if found not english return failure.
is_consider_only_uniqueURLdomain=true
is_consider_only_uniqueURLdomain.remarks=only first URL from a domain (say: "www.xyz.com/page1")consider. others ignore such as "www.xyz.com/page2"
curr_level=1
max_level=1
top_N_lines_to_skip_from_input_URL_file=836535
top_N_lines_to_skip_from_input_URL_file.remark=-1 if NO lines to skip.